{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: PySpark - II\n",
    "### CS186, UC Berkeley, Spring 2016\n",
    "### Due: Thursday Feb 25, 2016, 11:59 PM\n",
    "### Note: **This homework is to be done individually!  Do not modify any existing method signatures.**\n",
    "### **This is the second of two .ipynb files in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## On some computers it may be possible to run this lab \n",
    "## locally by using this script; you will need to run\n",
    "## this each time you start the notebook.\n",
    "## You do not need to run this on inst machines.\n",
    "\n",
    "# from local_install import setup_environment\n",
    "# setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from utils import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import CleanRDD\n",
    "from utils import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: CacheMap\n",
    "\n",
    "In this part, we'll construct an rdd that is backed by a `ClockMap` and will behave like `rdd.map(func)`.  \n",
    "First, implement the `ClockMap` class so that it maintains a cache (of limited `cacheSize`) using the clock replacement policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ClockMap:\n",
    "    \n",
    "    def __init__(self, cacheSize, func):\n",
    "        \"\"\"\n",
    "        Do not change existing variables.\n",
    "        [Optional] You are free to add additional items and methods.\n",
    "        \"\"\"\n",
    "        self.cacheSize = cacheSize\n",
    "        self.fn = func\n",
    "        self._p = 0 # pointer\n",
    "        self._increments = 0\n",
    "        self._miss_count = 0\n",
    "        self.buffers = [[None, 0] for x in range(cacheSize)]\n",
    "        self.items_to_index = {}\n",
    "        \n",
    "    def _increment(self):\n",
    "        \"\"\"\n",
    "        Do not change this method.\n",
    "        Updates the clock pointer. The modulo maintains the clock nature.\n",
    "        \"\"\"\n",
    "        self._increments += 1\n",
    "        self._p = (self._p + 1) % self.cacheSize\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"\n",
    "        Returns func(k) using the buffer to cache limited results.\n",
    "        \n",
    "        :param k: Value to be evaluated\n",
    "        \n",
    "        >>> clock = ClockMap(4, lambda x: x ** 2)\n",
    "        >>> clock[4]\n",
    "        16\n",
    "        >>> clock[3]\n",
    "        9\n",
    "        >>> clock._p\n",
    "        2\n",
    "        \"\"\"\n",
    "        # pass #TODO\n",
    "        \n",
    "        # Do it like in lecture??\n",
    "        # First check if in cache\n",
    "        for i in range(len(self.buffers)):\n",
    "            if self.buffers[i][0] == k:\n",
    "                # Check it\n",
    "                self.items_to_index[i] = True\n",
    "                return self.buffers[i][1]\n",
    "            \n",
    "        # Else, cache miss\n",
    "        self._miss_count += 1\n",
    "        \n",
    "        # Move pointer around, to find where to replace\n",
    "        while self._p in self.items_to_index and self.items_to_index.get(self._p):\n",
    "            # Second chance! Uncheck and move on\n",
    "            self.items_to_index[self._p] = False\n",
    "            self._increment()\n",
    "        \n",
    "        # Found where to replace, now replace. And don't forget to check it. \n",
    "        result = self.fn(k)\n",
    "        self.buffers[self._p] = [k, result]\n",
    "        self.items_to_index[self._p] = True\n",
    "        # I guess, according to test, also have to increment after writing.\n",
    "        self._increment()\n",
    "        return result\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `cacheMap`, which will return an rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cacheMap(rdd, cacheSize, func):\n",
    "    \"\"\"\n",
    "    Returns an RDD that behaves like rdd.map(func) but\n",
    "    is implemented using the ClockMap.\n",
    "    \n",
    "    :param rdd: Given RDD\n",
    "    :param cacheSize: Number of cache/buffer pages in the ClockMap\n",
    "    :param func: Function to map with\n",
    "    \"\"\"\n",
    "    # pass #TODO\n",
    "    \n",
    "    # Just use mapPartitionsWithIndex again? LOL\n",
    "    def iterate(y, itr):\n",
    "        clock = ClockMap(cacheSize, func)\n",
    "        for x in itr:\n",
    "            yield clock[x]\n",
    "            \n",
    "    \n",
    "    return rdd.mapPartitionsWithIndex(iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free test for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 9\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "clock = ClockMap(4, lambda x: x ** 2)\n",
    "print clock[4], clock[3]\n",
    "print clock._p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be \n",
    "```\n",
    "16, 9\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: External Algorithms\n",
    "\n",
    "You'll need an understanding of the partitioning step of external hashing, and the divide step of external sorting (recall the lecture on external algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import itertools\n",
    "import bisect\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some tools you may want to use (examples use cases included). You should Google the unfamiliar ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# itertools.islice\n",
    "generator = (y for y in range(100))\n",
    "test1 = itertools.islice(generator, 5)\n",
    "print next(test1)\n",
    "print next(test1)\n",
    "test2 = itertools.islice(generator, 5)\n",
    "print next(generator)\n",
    "print next(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heapq.merge\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "generator2 = (even for even in range(100)[::2])\n",
    "key = lambda x: x\n",
    "test2 = heapq.merge([generator1, generator2], key=key, reverse=False)\n",
    "next(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we insert 3, it goes to 1\n",
      "If we insert 1, it goes to 0\n",
      "If we insert 4, it goes to 1\n",
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# bisect.bisect_left\n",
    "buckets = [2, 4, 4]\n",
    "print \"If we insert 3, it goes to %d\" % bisect.bisect_left(buckets, 3)\n",
    "print \"If we insert 1, it goes to %d\" % bisect.bisect_left(buckets, 1)\n",
    "print \"If we insert 4, it goes to %d\" % bisect.bisect_left(buckets, 4)\n",
    "\n",
    "buckets2 = [(1, 2), (3, 4), (5, 6)]\n",
    "print bisect.bisect_left(buckets2, (0, 0))\n",
    "print bisect.bisect_left(buckets2, (5, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 16, 27, 28, 30, 59, 94, 97]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.sample\n",
    "rdd = sc.parallelize(range(100))\n",
    "fraction = 0.1\n",
    "rdd.sample(False, fraction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Serializer and os.unlink (Serializer is provided via utils.GeneralTools)\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "filename = \"temp\"\n",
    "with open(filename, \"w\") as f:\n",
    "    serializer.dump_stream(generator1, f)\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    stream = serializer.load_stream(f)\n",
    "    print next(stream)\n",
    "\n",
    "os.unlink(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_used_memory - returns an int in MB\n",
    "get_used_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to modify the following function - it should come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sort_dir(partId, n):\n",
    "    \"\"\"\n",
    "    Returns a path for temporary file.\n",
    "\n",
    "    :param n: Unique identification for file\n",
    "    \"\"\"\n",
    "    d = \"tmp/sort/\" + str(partId) + \"/\"\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    return os.path.join(d, str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def externalSortStream(iterator, partId=0, reverse=False, keyfunc=None, serial=serializer, limit=10, batch=100):\n",
    "    \"\"\"\n",
    "    Given an iterator, returns an iterator of sorted elements (according to parameters). \n",
    "    \n",
    "    :param iterator: iterator. Expects (Key, Value).\n",
    "    :param keyfunc: function applied on the keykey.\n",
    "    :param reverse: Reverse default ordering if true. (default is ascending; reverse is descending) \n",
    "    :param serializer: See README.\n",
    "    :param limit: memory limit.\n",
    "    :param batch: Number of elements to read at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_runs = [] # can be used to hold a list of iterators\n",
    "    all_runs_paths = []\n",
    "    run = [] # used to hold the current run of elements\n",
    "    length_c = 0\n",
    "    \n",
    "    def load(fileobj):\n",
    "        \"\"\"\n",
    "        Returns a generator object that outputs elements \n",
    "        from a serialized (saved) stream. Closes the file when done.\n",
    "        \n",
    "        :param fileobj: python object file\n",
    "        \"\"\"\n",
    "        for _ in serial.load_stream(fileobj):\n",
    "            yield _\n",
    "        fileobj.close()\n",
    "   \n",
    "    # TODO everywhere below \n",
    "    \n",
    "    \n",
    "    # Load up until hit soft limit\n",
    "    # Then sort the run\n",
    "    # Save run into a stream\n",
    "    # And then use heap merge to merge them\n",
    "    \n",
    "    # sorted() seems to take in a list, output list, okay\n",
    "    \n",
    "    \n",
    "    # Get all runs into all_runs\n",
    "    while True:\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Fill up a single run\n",
    "        while True:\n",
    "            \n",
    "            \n",
    "            c = list(itertools.islice(iterator, batch))\n",
    "            length_c = len(c)\n",
    "            # print(next(iterator))\n",
    "            \n",
    "#             for i in range(len(c)):\n",
    "#                 print(next(iterator))\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "#         if \"base case goes here\":\n",
    "#             break\n",
    "        \n",
    "        # Load up\n",
    "            run = run + c\n",
    "            #print(run)\n",
    "        \n",
    "        \n",
    "        # End case: hit memory limit, or nothing left to stream\n",
    "            if get_used_memory() > limit or length_c < batch: # TODO\n",
    "                break\n",
    "        \n",
    "        # Filled up a run\n",
    "        # Put it into a lambda function\n",
    "#         sorted_run = sorted(run, lambda x: keyfunc(x[0]))\n",
    "\n",
    "\n",
    "        sorted_run = sorted(run, key=lambda x: keyfunc(x[0]), reverse=reverse)\n",
    "        srun_path = get_sort_dir(partId, len(all_runs))\n",
    "        # with open(srun_path, \"w\") as f:\n",
    "        f = open(srun_path, \"w\")\n",
    "        serializer.dump_stream(sorted_run, f)\n",
    "        # Link into all_runs\n",
    "    \n",
    "    \n",
    "        # With ___ as ____ apparently closes the file\n",
    "        #with open(srun_path, \"r\") as f:\n",
    "            # f is the file obj\n",
    "        f = open(srun_path, \"r\")\n",
    "        all_runs.append(load(f))\n",
    "#             stream = serializer.load_stream(f)\n",
    "        \n",
    "        \n",
    "        all_runs_paths.append(srun_path)\n",
    "        \n",
    "        #os.unlink(srun_path)\n",
    "        \n",
    "        \n",
    "        # Keep making more runs, until nothing left to stream\n",
    "        if length_c < batch:\n",
    "            #print(True)\n",
    "            break;\n",
    "    \n",
    "        #print(False)\n",
    "        #print(length_c)\n",
    "        \n",
    "        # OH GOTTA CLEAR OUT THE RUN \n",
    "        run = []\n",
    "        \n",
    "        \n",
    "    # TODO some cleanup stuff\n",
    "    \n",
    "    # Delete all the runs! unlink\n",
    "    for path in all_runs_paths:\n",
    "        os.unlink(path)\n",
    "    \n",
    "    # Did all the runs! Now merge them\n",
    "    return heapq.merge(all_runs, key=lambda x: keyfunc(x[0]), reverse=reverse)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember to run the import box above.\n",
    "\n",
    "def partitionByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    Uses sampling to partitions the elements by the return value of \n",
    "    keyfunc.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "\n",
    "    if numPartitions is None:\n",
    "        numPartitions = rdd.getNumPartitions()\n",
    "\n",
    "    if numPartitions == 1:\n",
    "        if rdd.getNumPartitions() > 1:\n",
    "            rdd = rdd.coalesce(1)\n",
    "        return rdd\n",
    "    \n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    # What does getBuckets have to do with this though? \n",
    "    # Use these samples (maybe collect it?) to find (a list of) boundaries for each bucket.\n",
    "    # \n",
    "    \n",
    "    boundaries = getBuckets(rdd, ascending, numPartitions, keyfunc)\n",
    "    \n",
    "    print(\"boundaries = \" + str(boundaries))\n",
    "    \n",
    "    # You want to write a function that will bucket your data (think coarse partitioning) \n",
    "        # - given that you've calculated your buckets\n",
    "        # Just bisect.bisect_left will do? Should return index of insertion. \n",
    "    # Okay, so grab the boundary values, throw into list\n",
    "        # And then use bisect.bisect_left to figure out which bucket it should go into. Should be okay.\n",
    "        \n",
    "    \n",
    "    def bi(x):\n",
    "        print (\"x = \" + str(x))\n",
    "        print (\"index = \" + str(bisect.bisect_left(boundaries, x)))\n",
    "        return bisect.bisect_left(boundaries, x)\n",
    "        \n",
    "    #balanceLoad = lambda x: bisect.bisect_left(boundaries, x) # I dunno it didn't like it when I did x[0] so\n",
    "    balanceLoad = bi\n",
    "    \n",
    "    # Okay I think this is like\n",
    "        # balanceLoad tells you what...index? to go to\n",
    "        # So then hash by, everyone with same index into same partition. \n",
    "        \n",
    "    \n",
    "    # Omg let's just test out bisect\n",
    "    def iterate(y, itr):\n",
    "        for item in itr:\n",
    "            print(\"item = \" + str(item))\n",
    "            print(bisect.bisect_left(boundaries, item))\n",
    "            yield bisect.bisect_left(boundaries, item)\n",
    "\n",
    "    print(\"bisect results: \" + str(rdd.mapPartitionsWithIndex(iterate).collect()))\n",
    "    # Okay it looks like bisect.bisect_left is working right\n",
    "    # Just the rdd isn't PARTITIONING it right\n",
    "    #return rdd.partitionBy(numPartitions, func=bi)\n",
    "    print(\"right before partitioning, numPartitions = \" + str(numPartitions))\n",
    "\n",
    "\n",
    "    \n",
    "    thing = rdd.partitionBy(numPartitions, lambda x: bisect.bisect_left(boundaries, x))\n",
    "    \n",
    "    print(\"partitioned, has \" + str(thing.getNumPartitions()) + \" partitions\")\n",
    "    \n",
    "    def iterateAgain(y, itr):\n",
    "        for item in itr:\n",
    "            print(str(item))\n",
    "            yield item\n",
    "    print(\"results: \" + str(thing.mapPartitionsWithIndex(iterateAgain).collect()))\n",
    "\n",
    "    \n",
    "    return thing\n",
    "\n",
    "\n",
    "def getBuckets(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    [Optional] Returns a list of bucket boundaries of length (numPartitions - 1),\n",
    "    in an order as specfied by the given parameters: ascending, keyfunc. \n",
    "    Bucket boundaries are determined by sampling as specified in the README.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Try sampling about 10 per partition (expected value).\n",
    "    print(\"numPartitions = \" + str(numPartitions))\n",
    "    fraction = (10 * numPartitions) / rdd.count()\n",
    "        # You can do this using the given sample function (without replacement).\n",
    "    samples = rdd.sample(False, fraction).collect()\n",
    "        \n",
    "    # Use these samples (maybe collect it?) to find (a list of) boundaries for each bucket.\n",
    "    # But you have like 10x more than you want. \n",
    "    # So maybe sample, and then sort it, and then like divide by numPartitions, to grab the boundary values?\n",
    "#     sorted_samples = sorted(samples, key=keyfunc, reverse=(not ascending)) # sort by key\n",
    "    sorted_samples = sorted(samples, key=keyfunc)\n",
    "    print(\"sorted_samples length = \" + str(len(sorted_samples)))\n",
    "    # 2 partitions, want 1 bound value. So want numPartitions - 1 bound values. \n",
    "    \n",
    "    \n",
    "    \n",
    "    # OOHHHHH okay you only have 10x more than you want IF your rdd is big enough\n",
    "    # So gonna have to divide, actually. Not just skip by 10. \n",
    "    skip_by = len(sorted_samples) / numPartitions\n",
    "    if skip_by < 1:\n",
    "        skip_by = 1\n",
    "          \n",
    "    list = [sorted_samples[i][0] for i in range(skip_by - 1, len(sorted_samples) - 1, skip_by)]\n",
    "    \n",
    "    # list = [sorted_samples[i] for i in range(9, len(sorted_samples) - 1, 10)]\n",
    "    \n",
    "    \n",
    "    return list # I hope this does numPartitions - 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sortByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"\n",
    "    Returns an RDD after executing an external sort using \n",
    "    functions partitionByKey and externalSortStream. \n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyFunc: function to be applied to the key.\n",
    "    \n",
    "    \"\"\"\n",
    "    #pass # TODO\n",
    "    \n",
    "    def iterate(y, itr):\n",
    "        return externalSortStream(itr, y, not ascending, keyfunc)\n",
    "\n",
    "    \n",
    "    \n",
    "    return partitionByKey(rdd).mapPartitionsWithIndex(iterate)\n",
    "    #return rdd.mapPartitionsWithIndex(iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are tests for `partitionByKey` and `externalSortStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 7),\n",
       " (6, 6),\n",
       " (8, 8),\n",
       " (5, 5),\n",
       " (9, 9),\n",
       " (4, 4),\n",
       " (3, 3),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stream = ((i, i) for i in range(100))\n",
    "list(externalSortStream(test_stream, keyfunc=(lambda x: abs(50 - (x ** 2)))))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(7, 7),\n",
    " (6, 6),\n",
    " (8, 8),\n",
    " (5, 5),\n",
    " (9, 9),\n",
    " (4, 4),\n",
    " (3, 3),\n",
    " (2, 2),\n",
    " (1, 1),\n",
    " (0, 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "numPartitions = 4\n",
      "sorted_samples length = 20\n",
      "boundaries = [1, 2, 4]\n",
      "bisect results: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "right before partitioning, numPartitions = 4\n",
      "partitioned, has 4 partitions\n",
      "results: [(0, 0), (1, 1), (0, 12), (1, 3), (0, 28), (1, 21), (0, 18), (1, 25), (2, 8), (2, 2), (2, 24), (3, 27), (4, 30), (3, 15), (4, 14), (3, 9), (4, 16), (5, 23), (5, 5), (5, 17)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8, 3, 6, 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(20), 4).map(lambda x: (x * 37 % 6, x ** 3 % 34)))\n",
    "print(str(rdd.count()))\n",
    "#partitionByKey(rdd)\n",
    "\n",
    "def iterate(y, itr):\n",
    "    for item in itr:\n",
    "        yield (y, item)\n",
    "            \n",
    "    \n",
    "#return rdd.mapPartitionsWithIndex(iterate)\n",
    "# thing = partitionByKey(rdd, ascending=True, numPartitions=5)\n",
    "# print thing.getNumPartitions()\n",
    "# thing.mapPartitionsWithIndex(iterate).collect()\n",
    "\n",
    "\n",
    "\n",
    "# FOR KICKS\n",
    "newRdd = partitionByKey(rdd, ascending=True)\n",
    "def counterFunction(y, iterator):\n",
    "    count = 0\n",
    "    for item in iterator:\n",
    "        count += 1\n",
    "    yield count\n",
    "newRdd.mapPartitionsWithIndex(counterFunction).collect()\n",
    "\n",
    "\n",
    "# newRdd = partitionByKey(rdd, ascending=True)\n",
    "# def counterFunction(y, iterator):\n",
    "#     count = 0\n",
    "#     for item in iterator:\n",
    "#         yield (y, item)\n",
    "# newRdd.mapPartitionsWithIndex(counterFunction).collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#partitionByKey(rdd, ascending=True, numPartitions=5).collect()\n",
    "\n",
    "#partitionByKey(rdd, True, 5).collect()\n",
    "#print(partitionByKey(rdd).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look rather well-distributed. Try forcing a skewed distribution and observe how effective the partitioning is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a test for `sortByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-81, 81),\n",
       " (-83, 83),\n",
       " (-85, 85),\n",
       " (-87, 87),\n",
       " (-89, 89),\n",
       " (-91, 91),\n",
       " (-93, 93),\n",
       " (-95, 95),\n",
       " (-97, 97),\n",
       " (-99, 99)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(100), 4).map(lambda x: (x *((-1) ** x) , x)))\n",
    "sortByKey(rdd, keyfunc=lambda key: key, ascending=False).collect()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(-81, 81),\n",
    " (-83, 83),\n",
    " (-85, 85),\n",
    " (-87, 87),\n",
    " (-89, 89),\n",
    " (-91, 91),\n",
    " (-93, 93),\n",
    " (-95, 95),\n",
    " (-97, 97),\n",
    " (-99, 99)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: PASS - task3ClockMap.txt matched reference output.\n",
      "Task 3: PASS - task3CacheMap.txt matched reference output.\n",
      "Task 4: PASS - task4.txt matched reference output.\n"
     ]
    }
   ],
   "source": [
    "tests.test3ClockMap(ClockMap)\n",
    "tests.test3CacheMap(cacheMap)\n",
    "tests.test4(sortByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
