{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: PySpark - II\n",
    "### CS186, UC Berkeley, Spring 2016\n",
    "### Due: Thursday Feb 25, 2016, 11:59 PM\n",
    "### Note: **This homework is to be done individually!  Do not modify any existing method signatures.**\n",
    "### **This is the second of two .ipynb files in this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## On some computers it may be possible to run this lab \n",
    "## locally by using this script; you will need to run\n",
    "## this each time you start the notebook.\n",
    "## You do not need to run this on inst machines.\n",
    "\n",
    "# from local_install import setup_environment\n",
    "# setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from utils import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import CleanRDD\n",
    "from utils import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: CacheMap\n",
    "\n",
    "In this part, we'll construct an rdd that is backed by a `ClockMap` and will behave like `rdd.map(func)`.  \n",
    "First, implement the `ClockMap` class so that it maintains a cache (of limited `cacheSize`) using the clock replacement policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ClockMap:\n",
    "    \n",
    "    def __init__(self, cacheSize, func):\n",
    "        \"\"\"\n",
    "        Do not change existing variables.\n",
    "        [Optional] You are free to add additional items and methods.\n",
    "        \"\"\"\n",
    "        self.cacheSize = cacheSize\n",
    "        self.fn = func\n",
    "        self._p = 0 # pointer\n",
    "        self._increments = 0\n",
    "        self._miss_count = 0\n",
    "        self.buffers = [[None, 0] for x in range(cacheSize)]\n",
    "        self.items_to_index = {}\n",
    "        \n",
    "    def _increment(self):\n",
    "        \"\"\"\n",
    "        Do not change this method.\n",
    "        Updates the clock pointer. The modulo maintains the clock nature.\n",
    "        \"\"\"\n",
    "        self._increments += 1\n",
    "        self._p = (self._p + 1) % self.cacheSize\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"\n",
    "        Returns func(k) using the buffer to cache limited results.\n",
    "        \n",
    "        :param k: Value to be evaluated\n",
    "        \n",
    "        >>> clock = ClockMap(4, lambda x: x ** 2)\n",
    "        >>> clock[4]\n",
    "        16\n",
    "        >>> clock[3]\n",
    "        9\n",
    "        >>> clock._p\n",
    "        2\n",
    "        \"\"\"\n",
    "        \n",
    "        # First check if in cache\n",
    "        for i in range(len(self.buffers)):\n",
    "            if self.buffers[i][0] == k:\n",
    "                # Check the box\n",
    "                self.items_to_index[i] = True\n",
    "                return self.buffers[i][1]\n",
    "            \n",
    "        # Else, cache miss\n",
    "        self._miss_count += 1\n",
    "        \n",
    "        # Move pointer around, to find where to replace\n",
    "        while self._p in self.items_to_index and self.items_to_index.get(self._p):\n",
    "            # Second chance! Uncheck and move on\n",
    "            self.items_to_index[self._p] = False\n",
    "            self._increment()\n",
    "        \n",
    "        # Found where to replace, now replace. And don't forget to check it. \n",
    "        result = self.fn(k)\n",
    "        self.buffers[self._p] = [k, result]\n",
    "        self.items_to_index[self._p] = True\n",
    "        \n",
    "        self._increment()\n",
    "        return result\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `cacheMap`, which will return an rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cacheMap(rdd, cacheSize, func):\n",
    "    \"\"\"\n",
    "    Returns an RDD that behaves like rdd.map(func) but\n",
    "    is implemented using the ClockMap.\n",
    "    \n",
    "    :param rdd: Given RDD\n",
    "    :param cacheSize: Number of cache/buffer pages in the ClockMap\n",
    "    :param func: Function to map with\n",
    "    \"\"\"\n",
    "    \n",
    "    def iterate(y, itr):\n",
    "        clock = ClockMap(cacheSize, func)\n",
    "        for x in itr:\n",
    "            yield clock[x]\n",
    "            \n",
    "    return rdd.mapPartitionsWithIndex(iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free test for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 9\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "clock = ClockMap(4, lambda x: x ** 2)\n",
    "print clock[4], clock[3]\n",
    "print clock._p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be \n",
    "```\n",
    "16, 9\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: External Algorithms\n",
    "\n",
    "You'll need an understanding of the partitioning step of external hashing, and the divide step of external sorting (recall the lecture on external algorithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import itertools\n",
    "import bisect\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some tools you may want to use (examples use cases included). You should Google the unfamiliar ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# itertools.islice\n",
    "generator = (y for y in range(100))\n",
    "test1 = itertools.islice(generator, 5)\n",
    "print next(test1)\n",
    "print next(test1)\n",
    "test2 = itertools.islice(generator, 5)\n",
    "print next(generator)\n",
    "print next(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heapq.merge\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "generator2 = (even for even in range(100)[::2])\n",
    "key = lambda x: x\n",
    "test2 = heapq.merge([generator1, generator2], key=key, reverse=False)\n",
    "next(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we insert 3, it goes to 1\n",
      "If we insert 1, it goes to 0\n",
      "If we insert 4, it goes to 1\n",
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# bisect.bisect_left\n",
    "buckets = [2, 4, 4]\n",
    "print \"If we insert 3, it goes to %d\" % bisect.bisect_left(buckets, 3)\n",
    "print \"If we insert 1, it goes to %d\" % bisect.bisect_left(buckets, 1)\n",
    "print \"If we insert 4, it goes to %d\" % bisect.bisect_left(buckets, 4)\n",
    "\n",
    "buckets2 = [(1, 2), (3, 4), (5, 6)]\n",
    "print bisect.bisect_left(buckets2, (0, 0))\n",
    "print bisect.bisect_left(buckets2, (5, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 12, 16, 17, 22, 43, 45, 47, 52, 64, 81, 88, 97]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD.sample\n",
    "rdd = sc.parallelize(range(100))\n",
    "fraction = 0.1\n",
    "rdd.sample(False, fraction).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Serializer and os.unlink (Serializer is provided via utils.GeneralTools)\n",
    "generator1 = (odd for odd in range(100) if odd % 2)\n",
    "filename = \"temp\"\n",
    "with open(filename, \"w\") as f:\n",
    "    serializer.dump_stream(generator1, f)\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    stream = serializer.load_stream(f)\n",
    "    print next(stream)\n",
    "\n",
    "os.unlink(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_used_memory - returns an int in MB\n",
    "get_used_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to modify the following function - it should come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sort_dir(partId, n):\n",
    "    \"\"\"\n",
    "    Returns a path for temporary file.\n",
    "\n",
    "    :param n: Unique identification for file\n",
    "    \"\"\"\n",
    "    d = \"tmp/sort/\" + str(partId) + \"/\"\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "    return os.path.join(d, str(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * BEGIN STUDENT CODE *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def externalSortStream(iterator, partId=0, reverse=False, keyfunc=None, serial=serializer, limit=10, batch=100):\n",
    "    \"\"\"\n",
    "    Given an iterator, returns an iterator of sorted elements (according to parameters). \n",
    "    \n",
    "    :param iterator: iterator. Expects (Key, Value).\n",
    "    :param keyfunc: function applied on the keykey.\n",
    "    :param reverse: Reverse default ordering if true. (default is ascending; reverse is descending) \n",
    "    :param serializer: See README.\n",
    "    :param limit: memory limit.\n",
    "    :param batch: Number of elements to read at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_runs = [] # can be used to hold a list of iterators\n",
    "    all_runs_paths = []\n",
    "    run = [] # used to hold the current run of elements\n",
    "    length_c = 0\n",
    "    \n",
    "    def load(fileobj):\n",
    "        \"\"\"\n",
    "        Returns a generator object that outputs elements \n",
    "        from a serialized (saved) stream. Closes the file when done.\n",
    "        \n",
    "        :param fileobj: python object file\n",
    "        \"\"\"\n",
    "        for _ in serial.load_stream(fileobj):\n",
    "            yield _\n",
    "        fileobj.close()\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Get all runs into all_runs\n",
    "    while True:\n",
    "        \n",
    "        # Fill up a single run\n",
    "        while True:\n",
    "            \n",
    "            c = list(itertools.islice(iterator, batch))\n",
    "            length_c = len(c)\n",
    "        \n",
    "            # Load up\n",
    "            run = run + c\n",
    "        \n",
    "            # End case: hit memory limit, or nothing left to stream\n",
    "            if get_used_memory() > limit or length_c < batch: \n",
    "                break\n",
    "        \n",
    "        \n",
    "        # Sort run and save into stream.\n",
    "        sorted_run = sorted(run, key=lambda x: keyfunc(x[0]), reverse=reverse)\n",
    "        srun_path = get_sort_dir(partId, len(all_runs))\n",
    "        f = open(srun_path, \"w\")\n",
    "        serializer.dump_stream(sorted_run, f)\n",
    "        \n",
    "        # Link into all_runs\n",
    "        f = open(srun_path, \"r\")\n",
    "        all_runs.append(load(f))\n",
    "        all_runs_paths.append(srun_path)\n",
    "        \n",
    "\n",
    "        # Keep making more runs, until nothing left to stream\n",
    "        if length_c < batch:\n",
    "            break;\n",
    "    \n",
    "        # Clear out run\n",
    "        run = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    for path in all_runs_paths:\n",
    "        os.unlink(path)\n",
    "    \n",
    "    return heapq.merge(all_runs, key=lambda x: keyfunc(x[0]), reverse=reverse)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember to run the import box above.\n",
    "\n",
    "def partitionByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    Uses sampling to partitions the elements by the return value of \n",
    "    keyfunc.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "\n",
    "    if numPartitions is None:\n",
    "        numPartitions = rdd.getNumPartitions()\n",
    "\n",
    "    if numPartitions == 1:\n",
    "        if rdd.getNumPartitions() > 1:\n",
    "            rdd = rdd.coalesce(1)\n",
    "        return rdd\n",
    "    \n",
    "    \n",
    "    boundaries = getBuckets(rdd, ascending, numPartitions, keyfunc)\n",
    "    return rdd.partitionBy(numPartitions, lambda x: bisect.bisect_left(boundaries, x))\n",
    "\n",
    "\n",
    "def getBuckets(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"        \n",
    "    [Optional] Returns a list of bucket boundaries of length (numPartitions - 1),\n",
    "    in an order as specfied by the given parameters: ascending, keyfunc. \n",
    "    Bucket boundaries are determined by sampling as specified in the README.\n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyfunc: function to be applied to the key.\n",
    "    \"\"\"\n",
    "    # Base cases done.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sample about 10 per partition.\n",
    "    fraction = (10 * numPartitions) / rdd.count()\n",
    "    samples = rdd.sample(False, fraction).collect()\n",
    "        \n",
    "    sorted_samples = sorted(samples, key=keyfunc)\n",
    "    \n",
    "    skip_by = len(sorted_samples) / numPartitions\n",
    "    if skip_by < 1:\n",
    "        skip_by = 1\n",
    "          \n",
    "    return [sorted_samples[i][0] for i in range(skip_by - 1, len(sorted_samples) - 1, skip_by)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sortByKey(rdd, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "    \"\"\"\n",
    "    Returns an RDD after executing an external sort using \n",
    "    functions partitionByKey and externalSortStream. \n",
    "\n",
    "    :param ascending: Smallest first.\n",
    "    :param numPartitions: Number of partitions of the returning RDD.\n",
    "    :param keyFunc: function to be applied to the key.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def iterate(y, itr):\n",
    "        return externalSortStream(itr, y, not ascending, keyfunc)\n",
    "\n",
    "    \n",
    "    return partitionByKey(rdd).mapPartitionsWithIndex(iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * END STUDENT CODE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are tests for `partitionByKey` and `externalSortStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 7),\n",
       " (6, 6),\n",
       " (8, 8),\n",
       " (5, 5),\n",
       " (9, 9),\n",
       " (4, 4),\n",
       " (3, 3),\n",
       " (2, 2),\n",
       " (1, 1),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stream = ((i, i) for i in range(100))\n",
    "list(externalSortStream(test_stream, keyfunc=(lambda x: abs(50 - (x ** 2)))))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(7, 7),\n",
    " (6, 6),\n",
    " (8, 8),\n",
    " (5, 5),\n",
    " (9, 9),\n",
    " (4, 4),\n",
    " (3, 3),\n",
    " (2, 2),\n",
    " (1, 1),\n",
    " (0, 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd count = 20\n",
      "numPartitions = 5\n",
      "[8, 3, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(20), 4).map(lambda x: (x * 37 % 6, x ** 3 % 34)))\n",
    "print(\"rdd count = \" + str(rdd.count()))\n",
    "\n",
    "def iterate(y, itr):\n",
    "    for item in itr:\n",
    "        yield (y, item)\n",
    "            \n",
    "    \n",
    "print(\"numPartitions = \" + str(thing.getNumPartitions()))\n",
    "partitionByKey(rdd, ascending=True, numPartitions=5).mapPartitionsWithIndex(iterate).collect()\n",
    "\n",
    "\n",
    "\n",
    "# FOR KICKS\n",
    "newRdd = partitionByKey(rdd, ascending=True)\n",
    "def counterFunction(y, iterator):\n",
    "    count = 0\n",
    "    for item in iterator:\n",
    "        count += 1\n",
    "    yield count\n",
    "print(newRdd.mapPartitionsWithIndex(counterFunction).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look rather well-distributed. Try forcing a skewed distribution and observe how effective the partitioning is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a test for `sortByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-81, 81),\n",
       " (-83, 83),\n",
       " (-85, 85),\n",
       " (-87, 87),\n",
       " (-89, 89),\n",
       " (-91, 91),\n",
       " (-93, 93),\n",
       " (-95, 95),\n",
       " (-97, 97),\n",
       " (-99, 99)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = CleanRDD(sc.parallelize(range(100), 4).map(lambda x: (x *((-1) ** x) , x)))\n",
    "sortByKey(rdd, keyfunc=lambda key: key, ascending=False).collect()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Your output should be:\n",
    "```\n",
    "[(-81, 81),\n",
    " (-83, 83),\n",
    " (-85, 85),\n",
    " (-87, 87),\n",
    " (-89, 89),\n",
    " (-91, 91),\n",
    " (-93, 93),\n",
    " (-95, 95),\n",
    " (-97, 97),\n",
    " (-99, 99)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: PASS - task3ClockMap.txt matched reference output.\n",
      "Task 3: PASS - task3CacheMap.txt matched reference output.\n",
      "Task 4: PASS - task4.txt matched reference output.\n"
     ]
    }
   ],
   "source": [
    "tests.test3ClockMap(ClockMap)\n",
    "tests.test3CacheMap(cacheMap)\n",
    "tests.test4(sortByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
